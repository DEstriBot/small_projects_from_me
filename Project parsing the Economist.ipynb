{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93843836",
   "metadata": {},
   "source": [
    "- Parse the Economist web-site on topics by the keywords\n",
    "https://www.economist.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c34db32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b649a3f",
   "metadata": {},
   "source": [
    "# Write a class that does parsing:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69874f9",
   "metadata": {},
   "source": [
    "Hi!\n",
    "I've created a class that parser through the Economist web-site, gathers, provides links and downloads (the last 2 are optional).\n",
    "\n",
    "I'm just now starting my IT path and i can see that this code is far from perfect but i'm working hard on it.\n",
    "\n",
    "P.S. I am not sure if this project can be of any practical use, i've made it for fun and to exercise myself.\n",
    "Peace!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22a11e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class economist_parser:\n",
    "    \n",
    "    def __init__(self, root, section, init_page, end_page, **kwargs):\n",
    "        self.root = root\n",
    "        self.section = section\n",
    "        self.init_page = init_page\n",
    "        self.end_page = end_page\n",
    "        kwargs['root'] = self.root\n",
    "        kwargs['section'] = self.section\n",
    "\n",
    "    def status_checker (self):\n",
    "        \"\"\"\n",
    "        Checking the status of access \n",
    "        \"\"\"\n",
    "        url = f'{self.root}/{self.section}'\n",
    "        web_page = requests.get(url)\n",
    "        status = web_page.status_code\n",
    "        if status == 200:\n",
    "            print(f'Your status code is: {status}. That means access granted, now can get to parsing')\n",
    "        else:\n",
    "            print('Sorry, there\\'s some problems with access, try to parse the other way')\n",
    "            \n",
    "    def kw_definer(self):\n",
    "        \"\"\"\n",
    "        Forms a list of keywords to check from articles\n",
    "        Returns a list of keywords to iterate from\n",
    "        \"\"\"\n",
    "        self.keywords = input('Please enter the keywords, dividing them by a single comma: \\n')\n",
    "        self.list_of_kws = []\n",
    "        for i in self.keywords.split(','):\n",
    "            self.list_of_kws.append(i.strip().lower())\n",
    "        return self.list_of_kws\n",
    "    \n",
    "    def page_selector (self):\n",
    "        \"\"\"\n",
    "        Selecting a page to parse on and accessing it;\n",
    "        Selecting a body of the page with content and returning it\n",
    "        \"\"\"\n",
    "        page_link = f'{self.root}/{self.section}?page={self.n_page}'\n",
    "        soup = bs(requests.get(page_link).text, 'lxml')\n",
    "        self.content_body = soup.find('main', role='main')\n",
    "        return self.content_body\n",
    "    \n",
    "                \n",
    "    def article_downloader(self, download, head):\n",
    "        \"\"\"\n",
    "        On a selected page, chooses adequate articles, accesses them and downloads the contents\n",
    "        \n",
    "        \"\"\"\n",
    "        if download == None or download == False: # checking if the download is needed\n",
    "            pass\n",
    "        elif download == True:\n",
    "#             getting a link and accessing the page\n",
    "            topic_link = f'{self.root}{self.ref}'\n",
    "            soup = bs(requests.get(topic_link).text, 'lxml')\n",
    "            main_text = soup.find_all('p', class_='article__body-text')\n",
    "#             sacking the text of the article\n",
    "            transcript = ''\n",
    "            for n in range(len(main_text)):\n",
    "                transcript += f'\\t{main_text[n].get_text()} \\n\\n'\n",
    "#             deleting chars that won't fit the name\n",
    "            badchars = ['/', '?', '\\\\', '*']\n",
    "            for c in badchars:\n",
    "                head = head.replace(c, '')\n",
    "#             loading files\n",
    "            with open(f'{head}.docx', 'w', encoding='utf-8') as file:\n",
    "                file.write(transcript)\n",
    "            print('\\n============================================================\\nYour files are successfully downloaded! \\n============================================================\\n')\n",
    "\n",
    "    def shower(self, show, counter, head, descr):\n",
    "        \"\"\"\n",
    "        Returns a name, description and a link of the adequate articles\n",
    "        \"\"\"\n",
    "        if show == None or show == False:\n",
    "            pass\n",
    "        elif show == True:\n",
    "            print(counter, ') \\t', head, '\\n', descr, '\\n', f'{self.root}{self.ref}', '\\n')\n",
    "    \n",
    "    def parser_engine(self, show, download):\n",
    "        \"\"\"\n",
    "        Parsing mechanism; 3 main stages\n",
    "        1) Selecting a frame of the article and iterating through the frames; defining tags of different contents\n",
    "        2) Iterating through the list of kwords, checking and matching\n",
    "        3) Additional stuff - totaling, printing and returning results\n",
    "        \"\"\"\n",
    "        counter = 0\n",
    "        controller = []\n",
    "        new_frame = self.content_body.find_all('div', class_='css-e6sfh4 e1mrg8dy0')\n",
    "        for i in range(len(new_frame)):\n",
    "#         check if the description is not empty. Is it's not - parses, otherwise - skips the frame\n",
    "            if new_frame[i].find('p', class_='css-1tj7b79 e1dj3j7p0'):\n",
    "#         creating tags of parts of frames\n",
    "                head = self.content_body.find_all('h3')[i].get_text()\n",
    "                descr = self.content_body.find_all('p', class_='css-1tj7b79 e1dj3j7p0')[i].get_text()\n",
    "                self.ref = self.content_body.find_all('a')[i]['href']\n",
    "                for n in self.list_of_kws: # iterating through the kwords\n",
    "                    if head not in controller: # check if the article has already been counted\n",
    "                        if n in head.lower() or n in descr.lower(): # matching keywords and heads or descriptions of the article\n",
    "                            counter += 1\n",
    "                            controller.append(head)\n",
    "#                             optional functions\n",
    "                            self.shower(show, counter, head, descr)\n",
    "                            self.article_downloader(download, head)\n",
    "                        \n",
    "            else:\n",
    "                break   \n",
    "                    \n",
    "        self.total_counter += counter\n",
    "        \n",
    "        if counter == 1:\n",
    "            print(f'On the page {self.n_page} there\\'s {counter} article related to the topic\\n')\n",
    "        elif counter == 0:\n",
    "            print(f'On the page {self.n_page} there\\'s no articles related to the topic \\n')\n",
    "        else:\n",
    "            print(f'On the page {self.n_page} there\\'s {counter} articles related to the topic \\n')\n",
    "            \n",
    "    def related_articles_parser(self, show=None, total=None, download=None):\n",
    "        \"\"\"\n",
    "        Launches parsing mechanisms iterating through the chosen range of the pages\n",
    "        Launches additional methods - 'total' and 'show'\n",
    "        \"\"\"\n",
    "        self.kw_definer()\n",
    "        self.download = download\n",
    "        self.total_counter = 0\n",
    "        for self.n_page in range(self.init_page, self.end_page+1):\n",
    "            self.page_selector()\n",
    "            self.parser_engine(show, download)\n",
    "            \n",
    "        if total == None or total == False:\n",
    "            pass\n",
    "        elif total == True:\n",
    "            print(f'In total there\\'s {self.total_counter} related articles in {self.end_page} pages of the Economist {self.section} section')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7488c9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "economist = economist_parser('https://www.economist.com', 'business', 1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71099565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the keywords, dividing them by a single comma: \n",
      "Elon Musk, Musk, Twitter\n",
      "1 ) \t Elon Musk buys Twitter at last \n",
      " Now comes the hard part \n",
      " https://www.economist.com/business/2022/10/28/elon-musk-buys-twitter-at-last \n",
      "\n",
      "\n",
      "============================================================\n",
      "Your files are successfully downloaded! \n",
      "============================================================\n",
      "\n",
      "2 ) \t The reluctant rise of the diplomat CEO \n",
      " Elon Musk wants to be a statesman. Most bosses would rather not \n",
      " https://www.economist.com/business/2022/10/27/the-reluctant-rise-of-the-diplomat-ceo \n",
      "\n",
      "\n",
      "============================================================\n",
      "Your files are successfully downloaded! \n",
      "============================================================\n",
      "\n",
      "On the page 1 there's 2 articles related to the topic \n",
      "\n",
      "1 ) \t Will Elon Musk-owned Twitter end up as a “deal from hell”? \n",
      " Everything app. Or nothingburger \n",
      " https://www.economist.com/business/2022/10/11/will-elon-musk-owned-twitter-end-up-as-a-deal-from-hell \n",
      "\n",
      "\n",
      "============================================================\n",
      "Your files are successfully downloaded! \n",
      "============================================================\n",
      "\n",
      "2 ) \t Elon Musk is buying Twitter. Really. Probably \n",
      " The reluctant suitor avoids a trial, but inherits a world of commercial and legal woes \n",
      " https://www.economist.com/business/2022/10/05/elon-musk-is-buying-twitter-really-probably \n",
      "\n",
      "\n",
      "============================================================\n",
      "Your files are successfully downloaded! \n",
      "============================================================\n",
      "\n",
      "On the page 2 there's 2 articles related to the topic \n",
      "\n",
      "On the page 3 there's no articles related to the topic \n",
      "\n",
      "1 ) \t Twitter’s shareholders approve Elon Musk’s $44bn offer \n",
      " No duh \n",
      " https://www.economist.com/business/2022/09/15/twitters-shareholders-approve-elon-musks-44bn-offer \n",
      "\n",
      "\n",
      "============================================================\n",
      "Your files are successfully downloaded! \n",
      "============================================================\n",
      "\n",
      "On the page 4 there's 1 article related to the topic\n",
      "\n",
      "On the page 5 there's no articles related to the topic \n",
      "\n",
      "In total there's 5 related articles in 5 pages of the Economist business section\n"
     ]
    }
   ],
   "source": [
    "economist.related_articles_parser(total=True, show=True, download=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
